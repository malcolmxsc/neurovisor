use std::process::Stdio;

// Import our new modules
use neurovisor::vm::{spawn_firecracker, wait_for_api_socket, FirecrackerClient, to_absolute_path};
use neurovisor::ollama::OllamaClient;
use neurovisor::grpc::server::InferenceServer;
use neurovisor::grpc::inference::inference_service_server::InferenceServiceServer;
use neurovisor::grpc::VsockConnectedStream;

// In production, these would be loaded from a .env or config file
const API_SOCKET: &str = "/tmp/firecracker.socket";
const KERNEL_PATH: &str = "./vmlinuz";
const INITRAMFS_PATH: &str = "./initramfs";
const ROOTFS_PATH: &str = "./rootfs.ext4";
const VSOCK_PATH: &str = "./neurovisor.vsock";

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Production Standard: Use structured logging
    println!("[INFO] ðŸ“¸ INITIALIZING NEUROVISOR ORCHESTRATOR...");

    // 1. CLEANUP
    let _ = std::fs::remove_file(API_SOCKET);
    let _ = std::fs::remove_file(VSOCK_PATH);

    // 2. LAUNCH VMM
    let mut child = spawn_firecracker(API_SOCKET, Stdio::inherit())?;

    // 3. WAIT FOR API
    wait_for_api_socket(API_SOCKET, None)?;

    // 4. CREATE FIRECRACKER CLIENT
    let fc_client = FirecrackerClient::new(API_SOCKET);

    // 5. CONFIGURE VM: KERNEL, ROOTFS, AND VSOCK
    let kernel_abs = to_absolute_path(KERNEL_PATH)?;
    let initramfs_abs = to_absolute_path(INITRAMFS_PATH)?;
    let rootfs_abs = to_absolute_path(ROOTFS_PATH)?;

    println!("[INFO] ðŸ”§ CONFIGURING VM BOOT...");
    fc_client.boot_source(&kernel_abs, &initramfs_abs).await?;

    println!("[INFO] ðŸ’¾ ADDING ROOT DRIVE: {}", ROOTFS_PATH);
    fc_client.add_drive("root", &rootfs_abs, true, false).await?;

    println!("[INFO] ðŸ”Œ CONFIGURING VSOCK");
    fc_client.configure_vsock(3, VSOCK_PATH).await?;

    println!("[INFO] âš¡ STARTING VM");
    fc_client.start().await?;

    println!("[INFO] â³ WAITING FOR VM TO BOOT...");
    tokio::time::sleep(tokio::time::Duration::from_secs(3)).await;

    let ollama = OllamaClient::new("http://localhost:11434");
    let inference_server = InferenceServer::new(ollama);
    let service = InferenceServiceServer::new(inference_server);

    // Listen on vsock CID 2 (host), port 6000
    println!("[INFO] ðŸš€ STARTING GRPC SERVER ON VSOCK (CID 2, PORT 6000)...");
    let mut listener = tokio_vsock::VsockListener::bind(2, 6000)?;

    // Create a stream that wraps incoming connections
    let vsock_stream = async_stream::stream! {
        loop {
            match listener.accept().await {
                Ok((stream, _addr)) => {
                    yield Ok::<_, std::io::Error>(VsockConnectedStream(stream));
                }
                Err(e) => {
                    eprintln!("[ERROR] Failed to accept vsock connection: {}", e);
                }
            }
        }
    };

    // build and serve
    tonic::transport::Server::builder()
    .add_service(service)
    .serve_with_incoming(vsock_stream)
    .await?;

    child.kill()?; // Ensure VM doesn't become a zombie
    println!("[INFO] ðŸ›‘ ORCHESTRATOR EXIT CLEAN");
    Ok(())
}