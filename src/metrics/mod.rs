//! Prometheus metrics for NeuroVisor observability
//!
//! This module defines all metrics exposed at the /metrics endpoint.
//! Metrics are organized by layer: inference, gRPC, and cgroups.
//!
//! # Metric Types
//!
//! - **Counter**: Monotonically increasing value (requests, errors, tokens)
//! - **Gauge**: Value that can go up or down (in-flight requests, memory usage)
//! - **Histogram**: Distribution of values with buckets (latencies, sizes)
//!
//! # Usage
//!
//! ```ignore
//! use neurovisor::metrics::{REQUESTS_TOTAL, INFERENCE_DURATION};
//!
//! REQUESTS_TOTAL.with_label_values(&["llama3.2"]).inc();
//! INFERENCE_DURATION.observe(0.045); // 45ms
//! ```

use lazy_static::lazy_static;
use prometheus::{
    register_counter_vec, register_gauge, register_gauge_vec,
    register_histogram, register_histogram_vec,
    CounterVec, Gauge, GaugeVec, Histogram, HistogramVec,
    Encoder, TextEncoder,
};

// ─────────────────────────────────────────────────────────────────────────────
// Inference Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Total inference requests received
    /// Labels: model (e.g., "llama3.2")
    pub static ref REQUESTS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_requests_total",
        "Total number of inference requests",
        &["model"]
    ).unwrap();

    /// Total tokens generated across all requests
    pub static ref TOKENS_GENERATED_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_tokens_generated_total",
        "Total tokens generated by inference",
        &["model"]
    ).unwrap();

    /// Inference duration from Ollama (model processing time only)
    /// This is the time Ollama reports, not including gRPC overhead
    pub static ref INFERENCE_DURATION: Histogram = register_histogram!(
        "neurovisor_inference_duration_seconds",
        "Inference duration reported by Ollama",
        vec![0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    ).unwrap();

    /// Total errors during inference
    /// Labels: error_type (e.g., "ollama_error", "timeout")
    pub static ref ERRORS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_errors_total",
        "Total inference errors",
        &["error_type"]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// gRPC / Request Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Currently in-flight requests (gauge: can go up and down)
    pub static ref REQUESTS_IN_FLIGHT: Gauge = register_gauge!(
        "neurovisor_requests_in_flight",
        "Number of requests currently being processed"
    ).unwrap();

    /// Request prompt size in bytes
    pub static ref REQUEST_SIZE_BYTES: Histogram = register_histogram!(
        "neurovisor_request_size_bytes",
        "Size of inference request prompts in bytes",
        vec![100.0, 500.0, 1000.0, 5000.0, 10000.0, 50000.0, 100000.0]
    ).unwrap();

    /// Total gRPC request duration (end-to-end, including overhead)
    /// Compare with INFERENCE_DURATION to see gRPC overhead
    pub static ref GRPC_REQUEST_DURATION: HistogramVec = register_histogram_vec!(
        "neurovisor_grpc_request_duration_seconds",
        "Total gRPC request duration (end-to-end)",
        &["method"],  // "infer" or "infer_stream"
        vec![0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// VM Pool Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Number of pre-warmed VMs ready in the pool
    pub static ref POOL_WARM_VMS: Gauge = register_gauge!(
        "neurovisor_pool_warm_vms",
        "Number of pre-warmed VMs ready in pool"
    ).unwrap();

    /// Number of VMs currently handling requests
    pub static ref POOL_ACTIVE_VMS: Gauge = register_gauge!(
        "neurovisor_pool_active_vms",
        "Number of VMs currently handling requests"
    ).unwrap();

    /// Time to acquire a VM from the pool
    pub static ref VM_ACQUIRE_DURATION: Histogram = register_histogram!(
        "neurovisor_vm_acquire_seconds",
        "Time to acquire a VM from pool",
        vec![0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
    ).unwrap();

    /// Time to boot a new VM
    pub static ref VM_BOOT_DURATION: Histogram = register_histogram!(
        "neurovisor_vm_boot_seconds",
        "Time to boot a new VM",
        vec![0.5, 1.0, 2.0, 3.0, 5.0, 10.0, 15.0, 20.0, 30.0]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// cgroups Resource Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Memory usage per VM cgroup in bytes
    pub static ref CGROUP_MEMORY_USAGE: GaugeVec = register_gauge_vec!(
        "neurovisor_cgroup_memory_usage_bytes",
        "Current memory usage per VM cgroup",
        &["vm_id"]
    ).unwrap();

    /// CPU throttle count per VM cgroup
    /// How many times the VM was throttled due to CPU limits
    pub static ref CGROUP_CPU_THROTTLED: CounterVec = register_counter_vec!(
        "neurovisor_cgroup_cpu_throttled_total",
        "Number of times VM was CPU throttled",
        &["vm_id"]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// Agent Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Total agent tasks started
    pub static ref AGENT_TASKS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_agent_tasks_total",
        "Total number of agent tasks started",
        &["status"]  // "success", "error", "max_iterations"
    ).unwrap();

    /// Number of iterations per agent task
    pub static ref AGENT_ITERATIONS: Histogram = register_histogram!(
        "neurovisor_agent_iterations",
        "Number of iterations per agent task",
        vec![1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 15.0, 20.0]
    ).unwrap();

    /// Code execution duration in VM
    pub static ref CODE_EXECUTION_DURATION: Histogram = register_histogram!(
        "neurovisor_code_execution_seconds",
        "Duration of code execution in VM",
        vec![0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0]
    ).unwrap();

    /// Total code executions
    pub static ref CODE_EXECUTIONS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_code_executions_total",
        "Total code executions",
        &["language", "status"]  // status: "success", "error", "timeout"
    ).unwrap();

    /// LLM model load time (first call includes loading model into memory)
    pub static ref MODEL_LOAD_DURATION: HistogramVec = register_histogram_vec!(
        "neurovisor_model_load_seconds",
        "Time for first LLM call (includes model loading)",
        &["model"],
        vec![1.0, 5.0, 10.0, 30.0, 60.0, 90.0, 120.0, 180.0, 300.0]
    ).unwrap();

    /// Total tool calls made by agent
    pub static ref AGENT_TOOL_CALLS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_agent_tool_calls_total",
        "Total tool calls made by agent",
        &["tool"]  // "execute_code"
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// Metrics Export
// ─────────────────────────────────────────────────────────────────────────────

/// Encode all metrics in Prometheus text format
///
/// Returns a string suitable for HTTP response body at /metrics endpoint
pub fn encode_metrics() -> String {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = Vec::new();
    encoder.encode(&metric_families, &mut buffer).unwrap();
    String::from_utf8(buffer).unwrap()
}

/// Push all metrics to a Prometheus Pushgateway
///
/// This is the standard approach for batch/ephemeral jobs that complete
/// before Prometheus can scrape them. The job pushes metrics before exit,
/// and Prometheus scrapes the Pushgateway.
///
/// # Arguments
/// * `gateway_url` - Base URL of the Pushgateway (e.g., "http://localhost:9091")
/// * `job` - Job name for grouping metrics (e.g., "neurovisor_agent")
/// * `instance` - Optional instance label (e.g., trace_id)
///
/// # Example
/// ```ignore
/// push_to_gateway("http://localhost:9091", "neurovisor_agent", Some("trace-123")).await?;
/// ```
pub async fn push_to_gateway(
    gateway_url: &str,
    job: &str,
    instance: Option<&str>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    let metrics = encode_metrics();

    // Build URL: /metrics/job/{job}[/instance/{instance}]
    let url = match instance {
        Some(inst) => format!("{}/metrics/job/{}/instance/{}", gateway_url, job, inst),
        None => format!("{}/metrics/job/{}", gateway_url, job),
    };

    let client = reqwest::Client::new();
    let response = client
        .post(&url)
        .header("Content-Type", "text/plain")
        .body(metrics)
        .send()
        .await?;

    if !response.status().is_success() {
        let status = response.status();
        let body = response.text().await.unwrap_or_default();
        return Err(format!("Pushgateway returned {}: {}", status, body).into());
    }

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_metrics_register() {
        // Just verify metrics can be accessed without panic
        REQUESTS_TOTAL.with_label_values(&["test"]).inc();
        TOKENS_GENERATED_TOTAL.with_label_values(&["test"]).inc_by(10.0);
        INFERENCE_DURATION.observe(0.1);
        ERRORS_TOTAL.with_label_values(&["test_error"]).inc();
        REQUESTS_IN_FLIGHT.inc();
        REQUESTS_IN_FLIGHT.dec();
        REQUEST_SIZE_BYTES.observe(1000.0);
        GRPC_REQUEST_DURATION.with_label_values(&["infer"]).observe(0.05);
        CGROUP_MEMORY_USAGE.with_label_values(&["vm-1"]).set(1024.0);
        CGROUP_CPU_THROTTLED.with_label_values(&["vm-1"]).inc();
    }

    #[test]
    fn test_encode_metrics() {
        // Touch a metric first - Prometheus only exports metrics after first use
        REQUESTS_TOTAL.with_label_values(&["encode_test"]).inc();

        let output = encode_metrics();
        assert!(output.contains("neurovisor_requests_total"));
    }
}
