//! Prometheus metrics for NeuroVisor observability
//!
//! This module defines all metrics exposed at the /metrics endpoint.
//! Metrics are organized by layer: inference, gRPC, and cgroups.
//!
//! # Metric Types
//!
//! - **Counter**: Monotonically increasing value (requests, errors, tokens)
//! - **Gauge**: Value that can go up or down (in-flight requests, memory usage)
//! - **Histogram**: Distribution of values with buckets (latencies, sizes)
//!
//! # Usage
//!
//! ```ignore
//! use neurovisor::metrics::{REQUESTS_TOTAL, INFERENCE_DURATION};
//!
//! REQUESTS_TOTAL.with_label_values(&["llama3.2"]).inc();
//! INFERENCE_DURATION.observe(0.045); // 45ms
//! ```

use lazy_static::lazy_static;
use prometheus::{
    register_counter_vec, register_gauge, register_gauge_vec,
    register_histogram, register_histogram_vec,
    CounterVec, Gauge, GaugeVec, Histogram, HistogramVec,
    Encoder, TextEncoder,
};

// ─────────────────────────────────────────────────────────────────────────────
// Inference Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Total inference requests received
    /// Labels: model (e.g., "llama3.2")
    pub static ref REQUESTS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_requests_total",
        "Total number of inference requests",
        &["model"]
    ).unwrap();

    /// Total tokens generated across all requests
    pub static ref TOKENS_GENERATED_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_tokens_generated_total",
        "Total tokens generated by inference",
        &["model"]
    ).unwrap();

    /// Inference duration from Ollama (model processing time only)
    /// This is the time Ollama reports, not including gRPC overhead
    pub static ref INFERENCE_DURATION: Histogram = register_histogram!(
        "neurovisor_inference_duration_seconds",
        "Inference duration reported by Ollama",
        vec![0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    ).unwrap();

    /// Total errors during inference
    /// Labels: error_type (e.g., "ollama_error", "timeout")
    pub static ref ERRORS_TOTAL: CounterVec = register_counter_vec!(
        "neurovisor_errors_total",
        "Total inference errors",
        &["error_type"]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// gRPC / Request Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Currently in-flight requests (gauge: can go up and down)
    pub static ref REQUESTS_IN_FLIGHT: Gauge = register_gauge!(
        "neurovisor_requests_in_flight",
        "Number of requests currently being processed"
    ).unwrap();

    /// Request prompt size in bytes
    pub static ref REQUEST_SIZE_BYTES: Histogram = register_histogram!(
        "neurovisor_request_size_bytes",
        "Size of inference request prompts in bytes",
        vec![100.0, 500.0, 1000.0, 5000.0, 10000.0, 50000.0, 100000.0]
    ).unwrap();

    /// Total gRPC request duration (end-to-end, including overhead)
    /// Compare with INFERENCE_DURATION to see gRPC overhead
    pub static ref GRPC_REQUEST_DURATION: HistogramVec = register_histogram_vec!(
        "neurovisor_grpc_request_duration_seconds",
        "Total gRPC request duration (end-to-end)",
        &["method"],  // "infer" or "infer_stream"
        vec![0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// cgroups Resource Metrics
// ─────────────────────────────────────────────────────────────────────────────

lazy_static! {
    /// Memory usage per VM cgroup in bytes
    pub static ref CGROUP_MEMORY_USAGE: GaugeVec = register_gauge_vec!(
        "neurovisor_cgroup_memory_usage_bytes",
        "Current memory usage per VM cgroup",
        &["vm_id"]
    ).unwrap();

    /// CPU throttle count per VM cgroup
    /// How many times the VM was throttled due to CPU limits
    pub static ref CGROUP_CPU_THROTTLED: CounterVec = register_counter_vec!(
        "neurovisor_cgroup_cpu_throttled_total",
        "Number of times VM was CPU throttled",
        &["vm_id"]
    ).unwrap();
}

// ─────────────────────────────────────────────────────────────────────────────
// Metrics Export
// ─────────────────────────────────────────────────────────────────────────────

/// Encode all metrics in Prometheus text format
///
/// Returns a string suitable for HTTP response body at /metrics endpoint
pub fn encode_metrics() -> String {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = Vec::new();
    encoder.encode(&metric_families, &mut buffer).unwrap();
    String::from_utf8(buffer).unwrap()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_metrics_register() {
        // Just verify metrics can be accessed without panic
        REQUESTS_TOTAL.with_label_values(&["test"]).inc();
        TOKENS_GENERATED_TOTAL.with_label_values(&["test"]).inc_by(10.0);
        INFERENCE_DURATION.observe(0.1);
        ERRORS_TOTAL.with_label_values(&["test_error"]).inc();
        REQUESTS_IN_FLIGHT.inc();
        REQUESTS_IN_FLIGHT.dec();
        REQUEST_SIZE_BYTES.observe(1000.0);
        GRPC_REQUEST_DURATION.with_label_values(&["infer"]).observe(0.05);
        CGROUP_MEMORY_USAGE.with_label_values(&["vm-1"]).set(1024.0);
        CGROUP_CPU_THROTTLED.with_label_values(&["vm-1"]).inc();
    }

    #[test]
    fn test_encode_metrics() {
        // Touch a metric first - Prometheus only exports metrics after first use
        REQUESTS_TOTAL.with_label_values(&["encode_test"]).inc();

        let output = encode_metrics();
        assert!(output.contains("neurovisor_requests_total"));
    }
}
