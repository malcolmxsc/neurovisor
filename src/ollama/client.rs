//! Ollama API client for LLM inference

use futures_util::stream::{Stream, StreamExt};
use std::pin::Pin;

/// Client for interacting with Ollama's HTTP API
#[derive(Clone)]
pub struct OllamaClient {
    base_url: String,
    client: reqwest::Client,
}

impl OllamaClient {
    /// Create a new Ollama client
    ///
    /// # Arguments
    /// * `base_url` - The base URL of the Ollama server (e.g., "http://localhost:11434")
    pub fn new(base_url: impl Into<String>) -> Self {
        Self {
            base_url: base_url.into(),
            client: reqwest::Client::new(),
        }
    }

    /// Generate a streaming response from Ollama
    ///
    /// # Arguments
    /// * `prompt` - The input prompt for the LLM
    /// * `model` - The model name (e.g., "llama3.2")
    ///
    /// # Returns
    /// A stream of token strings as they're generated by the model
    pub async fn generate_stream(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
    ) -> Result<
        Pin<Box<dyn Stream<Item = Result<String, Box<dyn std::error::Error + Send + Sync>>> + Send>>,
        Box<dyn std::error::Error + Send + Sync>,
    > {
        let endpoint = format!("{}/api/generate", self.base_url);
        let prompt = prompt.into();
        let model = model.into();

        // Send the request to Ollama
        let bytes_stream = self
            .client
            .post(&endpoint)
            .json(&serde_json::json!({
                "model": model,
                "prompt": prompt,
                "stream": true
            }))
            .send()
            .await?
            .bytes_stream();

        // Transform the byte stream into a token stream
        let token_stream = bytes_stream.map(|chunk_result| {
            match chunk_result {
                Ok(bytes) => {
                    // Parse the JSON chunk
                    match serde_json::from_slice::<serde_json::Value>(&bytes) {
                        Ok(data) => {
                            // Extract the "response" field (the token)
                            if let Some(token) = data["response"].as_str() {
                                Ok(token.to_string())
                            } else {
                                // No token in this chunk, return empty string
                                Ok(String::new())
                            }
                        }
                        Err(e) => Err(Box::new(e) as Box<dyn std::error::Error + Send + Sync>),
                    }
                }
                Err(e) => Err(Box::new(e) as Box<dyn std::error::Error + Send + Sync>),
            }
        });

        Ok(Box::pin(token_stream))
    }

    /// Generate a complete (non-streaming) response from Ollama
    ///
    /// # Arguments
    /// * `prompt` - The input prompt for the LLM
    /// * `model` - The model name (e.g., "llama3.2")
    ///
    /// # Returns
    /// The complete generated text
    pub async fn generate(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
    ) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
        let mut stream = self.generate_stream(prompt, model).await?;
        let mut full_response = String::new();

        while let Some(token_result) = stream.next().await {
            match token_result {
                Ok(token) => full_response.push_str(&token),
                Err(e) => return Err(e),
            }
        }

        Ok(full_response)
    }
}
