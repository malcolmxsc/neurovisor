//! Ollama API client for LLM inference

use futures_util::stream::{Stream, StreamExt};
use std::pin::Pin;

/// Response from Ollama's generate endpoint with metadata
#[derive(Debug, Clone)]
pub struct GenerateResponse {
    /// The generated text
    pub response: String,
    /// Number of tokens generated
    pub eval_count: u32,
    /// Number of tokens in the prompt
    pub prompt_eval_count: u32,
    /// Time spent generating tokens (nanoseconds)
    pub eval_duration_ns: u64,
}

/// Client for interacting with Ollama's HTTP API
#[derive(Clone)]
pub struct OllamaClient {
    base_url: String,
    client: reqwest::Client,
}

impl OllamaClient {
    /// Create a new Ollama client
    ///
    /// # Arguments
    /// * `base_url` - The base URL of the Ollama server (e.g., "http://localhost:11434")
    pub fn new(base_url: impl Into<String>) -> Self {
        Self {
            base_url: base_url.into(),
            client: reqwest::Client::new(),
        }
    }

    /// Generate a streaming response from Ollama
    ///
    /// # Arguments
    /// * `prompt` - The input prompt for the LLM
    /// * `model` - The model name (e.g., "llama3.2")
    ///
    /// # Returns
    /// A stream of token strings as they're generated by the model
    pub async fn generate_stream(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
    ) -> Result<
        Pin<Box<dyn Stream<Item = Result<String, Box<dyn std::error::Error + Send + Sync>>> + Send>>,
        Box<dyn std::error::Error + Send + Sync>,
    > {
        let endpoint = format!("{}/api/generate", self.base_url);
        let prompt = prompt.into();
        let model = model.into();

        // Send the request to Ollama
        let bytes_stream = self
            .client
            .post(&endpoint)
            .json(&serde_json::json!({
                "model": model,
                "prompt": prompt,
                "stream": true
            }))
            .send()
            .await?
            .bytes_stream();

        // Transform the byte stream into a token stream
        let token_stream = bytes_stream.map(|chunk_result| {
            match chunk_result {
                Ok(bytes) => {
                    // Parse the JSON chunk
                    match serde_json::from_slice::<serde_json::Value>(&bytes) {
                        Ok(data) => {
                            // Extract the "response" field (the token)
                            if let Some(token) = data["response"].as_str() {
                                Ok(token.to_string())
                            } else {
                                // No token in this chunk, return empty string
                                Ok(String::new())
                            }
                        }
                        Err(e) => Err(Box::new(e) as Box<dyn std::error::Error + Send + Sync>),
                    }
                }
                Err(e) => Err(Box::new(e) as Box<dyn std::error::Error + Send + Sync>),
            }
        });

        Ok(Box::pin(token_stream))
    }

    /// Generate a complete (non-streaming) response from Ollama
    ///
    /// # Arguments
    /// * `prompt` - The input prompt for the LLM
    /// * `model` - The model name (e.g., "llama3.2")
    ///
    /// # Returns
    /// GenerateResponse with the text and token/timing metadata
    pub async fn generate(
        &self,
        prompt: impl Into<String>,
        model: impl Into<String>,
    ) -> Result<GenerateResponse, Box<dyn std::error::Error + Send + Sync>> {
        let endpoint = format!("{}/api/generate", self.base_url);
        let prompt = prompt.into();
        let model = model.into();

        let mut bytes_stream = self
            .client
            .post(&endpoint)
            .json(&serde_json::json!({
                "model": model,
                "prompt": prompt,
                "stream": true
            }))
            .send()
            .await?
            .bytes_stream();

        let mut full_response = String::new();
        let mut eval_count = 0u32;
        let mut prompt_eval_count = 0u32;
        let mut eval_duration_ns = 0u64;

        while let Some(chunk_result) = bytes_stream.next().await {
            let bytes = chunk_result?;
            if let Ok(data) = serde_json::from_slice::<serde_json::Value>(&bytes) {
                // Accumulate response tokens
                if let Some(token) = data["response"].as_str() {
                    full_response.push_str(token);
                }

                // Check if this is the final chunk with metadata
                if data["done"].as_bool() == Some(true) {
                    eval_count = data["eval_count"].as_u64().unwrap_or(0) as u32;
                    prompt_eval_count = data["prompt_eval_count"].as_u64().unwrap_or(0) as u32;
                    eval_duration_ns = data["eval_duration"].as_u64().unwrap_or(0);
                }
            }
        }

        Ok(GenerateResponse {
            response: full_response,
            eval_count,
            prompt_eval_count,
            eval_duration_ns,
        })
    }
}
