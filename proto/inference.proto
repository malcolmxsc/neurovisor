syntax = "proto3";
package neurovisor.inference;

service InferenceService {
    // Server streaming: Send prompt, stream tokens back
    rpc InferStream(InferenceRequest) returns (stream TokenChunk);


    // Unary: Complete Response
    rpc Infer(InferenceRequest) returns (InferenceResponse);


}

message InferenceRequest {
    string prompt = 1;
    string model = 2;
    float temperature = 3;
    int32 max_tokens = 4;
    bool stream = 5;
    map <string, string> metadata = 6;
}

message TokenChunk {
    string token = 1;
    bool is_final = 2;
    int32 token_index = 3;
    InferenceMetadata metadata = 4;
}

message InferenceMetadata {
    int32  total_tokens = 1;
    double total_latency_ms = 2;
    string model = 3;
    string trace_id = 4;

}

message InferenceResponse {
    string response = 1;
    int32 tokens_generated = 2;
    double latency_ms = 3;
    string model_used = 4;
}
